"""
LLM ëª¨ë¸ ë¡œë”© ëª¨ë“ˆ - ì´ˆê¸° 1íšŒë§Œ ë¡œë“œë˜ê³  íŒŒì´í”„ë¼ì¸ì„ ì „ì—­ ì¬ì‚¬ìš©
"""

import torch
import time
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from functools import lru_cache

# âœ… ê¸€ë¡œë²Œ íŒŒì´í”„ë¼ì¸ ë³€ìˆ˜ (ì§ì ‘ ì‚¬ìš©ì€ ê¸ˆì§€, ë‚´ë¶€ì—ì„œë§Œ ê´€ë¦¬)
_global_llm_pipe = None


def _load_llm_pipeline(model_name="EleutherAI/polyglot-ko-1.3b"):
    """
    ë‚´ë¶€ìš©: LLM íŒŒì´í”„ë¼ì¸ì„ ë¡œë”©í•˜ëŠ” í•¨ìˆ˜. ì „ì—­ ë³€ìˆ˜ì— ì €ì¥.

    Args:
        model_name (str): ì‚¬ìš©í•  ëª¨ë¸ëª…

    Returns:
        transformers.pipeline: í…ìŠ¤íŠ¸ ìƒì„±ìš© íŒŒì´í”„ë¼ì¸
    """
    global _global_llm_pipe

    if _global_llm_pipe is not None:
        print("âœ… [LLM] ê¸°ì¡´ ëª¨ë¸ íŒŒì´í”„ë¼ì¸ ì¬ì‚¬ìš©")
        return _global_llm_pipe

    print("ğŸš€ [LLM] ëª¨ë¸ ë¡œë”© ì¤‘...")

    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
        device_map="auto" if torch.cuda.is_available() else None,
    )

    pipe = pipeline(
        "text-generation", model=model, tokenizer=tokenizer  # âœ… device ì œê±°
    )

    _global_llm_pipe = pipe
    print("âœ… [LLM] ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")

    return pipe


def initialize_llm(model_name="EleutherAI/polyglot-ko-1.3b"):
    """
    ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘ ì‹œ LLM íŒŒì´í”„ë¼ì¸ì„ ì´ˆê¸°í™”í•˜ëŠ” í•¨ìˆ˜ (ëª…ì‹œì  ì´ˆê¸°í™”)
    """
    start = time.time()
    _load_llm_pipeline(model_name)
    print(f"âœ… [LLM] ì´ˆê¸°í™” ì™„ë£Œ (ì†Œìš” ì‹œê°„: {time.time() - start:.2f}ì´ˆ)")


@lru_cache(maxsize=1)
def get_llm_pipeline():
    """
    ì™¸ë¶€ì—ì„œ ì‚¬ìš©í•˜ëŠ” LLM íŒŒì´í”„ë¼ì¸ ì œê³µ í•¨ìˆ˜ (ìë™ ì´ˆê¸°í™” í¬í•¨)

    Returns:
        transformers.pipeline: LLM í…ìŠ¤íŠ¸ ìƒì„±ìš© íŒŒì´í”„ë¼ì¸
    """
    return _load_llm_pipeline()


# ë…ë¦½ ì‹¤í–‰ ì‹œ í…ŒìŠ¤íŠ¸ìš©
if __name__ == "__main__":
    initialize_llm()
    print("âœ… LLM ëª¨ë“ˆ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
