"""
ì¥ì•  ì˜ˆì¸¡ í•µì‹¬ ëª¨ë“ˆ: ìœ ì‚¬ ì¥ì• ì‚¬ë¡€ë¥¼ ì°¾ê³  ì¥ì• ì  ë° ì¡°ì¹˜ì‚¬í•­ì„ ì¶”ë¡ 
"""

import os
import time
import re
import json
import asyncio
import aiohttp
import logging
import chromadb
from chromadb.utils import embedding_functions
from datetime import datetime
from functools import lru_cache
from rapidfuzz import process, fuzz
from typing import Dict, List, Tuple, Any, Optional, Set, Union

# ìœ í‹¸ë¦¬í‹° ëª¨ë“ˆ ì„í¬íŠ¸
from .fault_prediction_utils import (
    normalize_text,
    extract_alert_codes_cached,
    clean_alert_message,
    calculate_field_matching,
    identify_field_from_keywords,
    analyze_equipment_mentions,
    check_specialized_patterns,
    explain_equipment_hierarchy
)

# ìƒìˆ˜ ë¡œë“œ
from .fault_prediction_constants import (
    DEFAULT_PROMPT_START_MESSAGE,
    FIELD_KEYWORDS,
    EQUIPMENT_KEYWORDS,
    ALERT_TYPE_KEYWORDS
)

# ìƒìˆ˜ ì •ì˜ - íŒŒì¼ ìµœìƒë‹¨ì— ì¶”ê°€
ERROR_DB_ACCESS = "VECTOR_DB_ACCESS_ERROR"

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
)
logger = logging.getLogger(__name__)

# ìƒìˆ˜ ì •ì˜
API_BASE_URL = "http://localhost:80/api"
VECTOR_DB_DIR = "./chroma_db"
VECTOR_DB_NEW_DIR = "./chroma_db_new"
EMBEDDING_MODEL = "intfloat/multilingual-e5-base"

HTML_NBSP_3 = "&nbsp&nbsp&nbsp"

# ì „ì—­ ë³€ìˆ˜
_guksa_id = ''
_vector_search_cache = {}
_VECTOR_CACHE_SIZE = 50
_VECTOR_CACHE_EXPIRY = 3600  # 1ì‹œê°„
_collection_instance = None

# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜


def set_guksa_id(guksa_id):
    """êµ­ì‚¬ ID ì„¤ì • í•¨ìˆ˜"""
    global _guksa_id
    _guksa_id = guksa_id
    return guksa_id


def get_guksa_id():
    """êµ­ì‚¬ ID ì¡°íšŒ í•¨ìˆ˜"""
    global _guksa_id
    return _guksa_id

# ë¹„ë™ê¸° ì™¸ë¶€ API í†µì‹  í•¨ìˆ˜ë“¤


async def fetch_external_info_all_async(endpoint: str, method: str = "post", data: dict = None):
    """íŠ¹ì • endpointì— ëŒ€í•´ ì „ì²´ ì‘ë‹µì„ ë°˜í™˜í•˜ëŠ” ë¹„ë™ê¸° í•¨ìˆ˜"""
    try:
        url = f"{API_BASE_URL}/{endpoint}"
        timeout = aiohttp.ClientTimeout(total=2, connect=1)

        async with aiohttp.ClientSession(timeout=timeout) as session:
            if method.lower() == "post":
                async with session.post(url, json=data) as response:
                    if response.status != 200:
                        return {}
                    json_data = await response.json()
            else:
                async with session.get(url) as response:
                    if response.status != 200:
                        return {}
                    json_data = await response.json()

            return json_data.get("response", json_data)

    except (aiohttp.ClientError, KeyError, json.JSONDecodeError, asyncio.TimeoutError) as e:
        logger.warning(f"ì™¸ë¶€ API í˜¸ì¶œ ì˜¤ë¥˜: {endpoint} - {str(e)}")
        return {}


async def fetch_external_info_async(endpoint: str, key: str, method: str = "post", data: dict = None):
    """ë¹„ë™ê¸° ì™¸ë¶€ API í˜¸ì¶œ - íŠ¹ì • í‚¤ ê°’ë§Œ ë°˜í™˜"""
    try:
        url = f"{API_BASE_URL}/{endpoint}"
        timeout = aiohttp.ClientTimeout(total=2, connect=1)

        async with aiohttp.ClientSession(timeout=timeout) as session:
            if method.lower() == "post":
                async with session.post(url, json=data) as response:
                    if response.status != 200:
                        return 0
                    json_data = await response.json()
            else:
                async with session.get(url) as response:
                    if response.status != 200:
                        return 0
                    json_data = await response.json()

            return json_data["response"].get(key, 0) if "response" in json_data else json_data.get(key, 0)

    except (aiohttp.ClientError, KeyError, json.JSONDecodeError, asyncio.TimeoutError) as e:
        logger.warning(f"ì™¸ë¶€ API í‚¤ ì¡°íšŒ ì˜¤ë¥˜: {endpoint}/{key} - {str(e)}")
        return 0


async def fetch_external_factors_async(guksa_id=None):
    """ì™¸ë¶€ ìš”ì¸ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” ë¹„ë™ê¸° í•¨ìˆ˜ - MW í˜ì´ë”©, ì „ì› ìƒíƒœ, ì¼€ì´ë¸” ìƒíƒœ"""
    gid = guksa_id or get_guksa_id()

    # ëª¨ë“  API í˜¸ì¶œì„ ë™ì‹œì— ì‹¤í–‰
    tasks = [
        fetch_external_info_all_async("mw_info", "post", {"guksa_id": gid}),
        fetch_external_info_async(
            "cable_status", "unrecovered_alarm", "get", {"guksa_id": gid})
    ]

    results = await asyncio.gather(*tasks)
    mw_info, cable_damage_count = results

    fading_count = mw_info.get("fading_count", 0)
    power_outage_count = mw_info.get("battery_mode_count", 0)

    return {
        "fading_count": fading_count,
        "power_outage_count": power_outage_count,
        "cable_damage_count": cable_damage_count
    }

# ë©”ì¸ ì¿¼ë¦¬ í•¨ìˆ˜


async def run_query(mode, query, user_id="default_user"):
    """ì‚¬ìš©ì ì¿¼ë¦¬ë¥¼ ì²˜ë¦¬í•˜ì—¬ ìœ ì‚¬ ì¥ì• ì‚¬ë¡€ë¥¼ ê²€ìƒ‰í•˜ê³  ì¢…í•© ì˜ê²¬ì„ ìƒì„±í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜ (ë¹„ë™ê¸° ë²„ì „)"""
    start_time = time.time()

    # ë²¡í„° DB ì»¬ë ‰ì…˜ ê°€ì ¸ì˜¤ê¸°
    collection, error = get_vector_db_collection()
    if error:
        # ì˜¤ë¥˜ íƒ€ì…ìœ¼ë¡œ ë¹„êµ
        error_msg = error["message"] if isinstance(error, dict) else str(error)
        error_type = error.get("type") if isinstance(error, dict) else None

        # ë²¡í„°DB ì ‘ê·¼ ì˜¤ë¥˜ì¸ ê²½ìš°
        if error_type == ERROR_DB_ACCESS:
            opinion_msg = f"âŒ ì˜¤ë¥˜: {error_msg}"
        else:
            opinion_msg = f"âŒ ì˜¤ë¥˜: ì‹œìŠ¤í…œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {error_msg}"

        return {
            "opinion": opinion_msg,
            "summary": [],
            "details": [],
            "processing_time": 0,
            "error": error_msg
        }

    # í”„ë¡¬í”„íŠ¸ ì‹œì‘ ë©”ì‹œì§€ ì¶”ê°€
    if not query.startswith(DEFAULT_PROMPT_START_MESSAGE):
        query = DEFAULT_PROMPT_START_MESSAGE + query

    # ë³‘ë ¬ë¡œ ë²¡í„° ê²€ìƒ‰ ë° ì™¸ë¶€ ìš”ì¸ ê°€ì ¸ì˜¤ê¸°
    external_factors_task = fetch_external_factors_async(get_guksa_id())

    # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ìˆ˜í–‰ (ë²¡í„° + í‚¤ì›Œë“œ + íŒ¨í„´)
    sorted_results, search_results = await hybrid_search_async(query, collection)

    # ì™¸ë¶€ ìš”ì¸ ê²°ê³¼ ê¸°ë‹¤ë¦¬ê¸°
    external_factors = await external_factors_task

    # ê²°ê³¼ê°€ ì—†ì„ ê²½ìš° ì²˜ë¦¬
    if not sorted_results:
        return json.dumps({
            "error": "ì…ë ¥í•˜ì‹  ë‚´ìš©ê³¼ ìœ ì‚¬í•œ ì¥ì•  ì‚¬ë¡€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë” êµ¬ì²´ì ì¸ ì¥ì•  ìƒí™©ì´ë‚˜ ê²½ë³´(ì•ŒëŒ,ë¡œê·¸) ë‚´ì—­ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”."
        }, ensure_ascii=False)

    # ìƒìœ„ ê²°ê³¼ ì¶”ì¶œ
    top_results = sorted_results[:3]

    # ê²°ê³¼ ë°ì´í„° êµ¬ì„± - ë¨¼ì € ì‹¤í–‰í•˜ì—¬ ì‹ ë¢°ë„ í™•ë³´
    summary_rows = build_summary_rows(top_results)
    details = build_details(top_results)

    # ì¥ì• ì  ì¶”ë¡  2 - ìœ ì‚¬ ì‚¬ë¡€ ê¸°ë°˜ (ë¨¼ì € ìˆ˜í–‰í•˜ì—¬ ì¶”ë¡ 1ê³¼ ë¹„êµì— ì‚¬ìš©)
    fault_infer_2 = predict_fault_real_cases(top_results)

    # summary_rowsì˜ ì²« ë²ˆì§¸ í–‰(1ìˆœìœ„)ì˜ ì‹ ë¢°ë„ë¥¼ ì¥ì• ì  ì¶”ë¡ 2ì˜ ì‹ ë¢°ë„ë¡œ ì‚¬ìš©
    if summary_rows and len(summary_rows) > 0:
        top_confidence_str = summary_rows[0]["ì‹ ë¢°ë„"]
        if top_confidence_str.endswith('%'):
            top_confidence = float(top_confidence_str.replace('%', ''))
            fault_infer_2["ì‹ ë¢°ë„"] = top_confidence

    # ì¥ì• ì  ì¶”ë¡  1 - ê²½ë³´/ì¦ìƒ íŒ¨í„´ ê¸°ë°˜
    fault_infer_1 = await predict_fault_patterns(query, top_results, external_factors)

    # ì¥ì• ì  ì¶”ë¡ 1ê³¼ ì¶”ë¡ 2ì˜ ì‹ ë¢°ë„ê°€ ê°™ì€ ê²½ìš°, ì˜ë„ì ìœ¼ë¡œ ë‹¤ë¥´ê²Œ ì¡°ì •
    if fault_infer_1.get("ì‹ ë¢°ë„") == fault_infer_2.get("ì‹ ë¢°ë„"):
        # ì¶”ë¡ 1ì˜ ì‹ ë¢°ë„ë¥¼ ì•½ê°„ ë‹¤ë¥´ê²Œ ì¡°ì • (ìµœì†Œ 0.2% ì°¨ì´)
        adjusted_confidence = fault_infer_1.get("ì‹ ë¢°ë„") + 0.7
        # 100%ë¥¼ ë„˜ì§€ ì•Šë„ë¡ ë³´ì •
        if adjusted_confidence > 100:
            adjusted_confidence = fault_infer_1.get("ì‹ ë¢°ë„") - 0.7
        fault_infer_1["ì‹ ë¢°ë„"] = round(adjusted_confidence, 1)

    # ì¢…í•© ì˜ê²¬ ìƒì„±
    comprehensive_opinion = await generate_brief_async(
        query, top_results, external_factors, fault_infer_2, fault_infer_1
    )

    # ê²°ê³¼ JSON êµ¬ì„±
    result_dict = {
        "opinion": comprehensive_opinion,
        "summary": summary_rows,
        "details": details,
        "processing_time": time.time() - start_time
    }

    # opinionì´ ë¹„ì–´ìˆìœ¼ë©´ ê¸°ë³¸ ì•ˆë‚´ ë©”ì‹œì§€ë¡œ ëŒ€ì²´
    if not result_dict["opinion"]:
        result_dict["opinion"] = "ìœ ì‚¬í•œ ì¥ì• ì‚¬ë¡€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë” êµ¬ì²´ì ì¸ ë‚´ìš©ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."

    return result_dict


async def predict_fault_patterns(query, top_results, external_factors):
    """ì¥ì• ì  ì¶”ë¡  1: ê²½ë³´/ì¦ìƒ íŒ¨í„´ ê¸°ë°˜ ì¥ì• ì  ì˜ˆì¸¡ (ê°œì„ ëœ ë²„ì „)"""
    # 1. ê²½ë³´ ë° ì¦ìƒ ì¶”ì¶œ
    cleaned_query = clean_alert_message(query)

    # 2. ë¶„ì•¼ë³„ ì¥ë¹„ ë°œê²¬ ë¹ˆë„ ë¶„ì„
    field_equipment_counts = analyze_field_equipment_mentions(query)

    # 3. ì¥ë¹„ ê³„ì¸µ êµ¬ì¡° ë¶„ì„
    equipment_analysis = analyze_equipment_mentions(
        query, field_equipment_counts)

    # 4. ì™¸ë¶€ ìš”ì¸ ë¶„ì„
    fading_count = external_factors.get("fading_count", 0)
    power_outage_count = external_factors.get("power_outage_count", 0)
    cable_damage_count = external_factors.get("cable_damage_count", 0)

    # 5. ì¶”ë¡  ë¡œì§ ì ìš©

    # 5.1 ì„ ë¡œ(ê´‘ì¼€ì´ë¸”) ì¥ì•  ì¶”ë¡ 
    if cable_damage_count > 0 or "ê´‘ì¼€ì´ë¸” í”¼í•´" in cleaned_query or "ê´‘ì¼€ì´ë¸” ì¥ì• " in cleaned_query:
        if sum(field_equipment_counts.values()) >= 3:  # 3ê°œ ì´ìƒ ë¶„ì•¼ì—ì„œ ì¥ë¹„ ì–¸ê¸‰
            return {
                "ì¥ì• ì ": "ê´‘ì¼€ì´ë¸” ì¥ì• ",
                "ì¥ì• ë¶„ì•¼": "ì„ ë¡œ",
                "ì‹ ë¢°ë„": 85.5,  # ì†Œìˆ˜ì ìœ¼ë¡œ ë³€ê²½
                "ê·¼ê±°": [
                    f"ë¯¸ë³µêµ¬ëœ ì„ ë¡œ ì¥ì• ê°€ {cable_damage_count}ê±´ ë°œê²¬ë¨" if cable_damage_count > 0 else "ë‹¤ìˆ˜ì˜ ë¶„ì•¼ë³„ ì¥ë¹„ì—ì„œ ê²½ë³´ ë°œìƒ",
                    "ê´‘ì¼€ì´ë¸” ê´€ë ¨ í‚¤ì›Œë“œ ë°œê²¬" if "ê´‘ì¼€ì´ë¸” ì¥ì• " in cleaned_query else "ì—¬ëŸ¬ ë¶„ì•¼ ì¥ë¹„ ë™ì‹œ ì¥ì• ëŠ” ì„ ë¡œ ë¬¸ì œì˜ íŠ¹ì§•"
                ],
                "íŒ¨í„´_ê·¼ê±°": "ìƒìœ„ ì¥ë¹„ì— ì—°ê²°ëœ í•˜ìœ„ ì¥ë¹„ë“¤ì˜ ê²½ë³´ê°€ ë‹¤ìˆ˜ ë°œìƒ"
            }

    # 5.2 MW í˜ì´ë”© ì¶”ë¡ 
    if fading_count > 0 or "í˜ì´ë”©" in cleaned_query or "fading" in cleaned_query.lower():
        # MW í˜ì´ë”©ì€ MW ìì²´ ê²½ë³´ ì—†ì´ ì—°ê²°ëœ ì¥ë¹„ë“¤ì—ì„œ ê²½ë³´ ë°œìƒ
        if field_equipment_counts.get("IP", 0) > 0 or field_equipment_counts.get("ì „ì†¡", 0) > 0:
            return {
                "ì¥ì• ì ": "MW í˜ì´ë”© í˜„ìƒ",
                "ì¥ì• ë¶„ì•¼": "MW",
                "ì‹ ë¢°ë„": 79.3,  # ì†Œìˆ˜ì ìœ¼ë¡œ ë³€ê²½
                "ê·¼ê±°": [
                    f"MW ì¥ë¹„ ì¤‘ ë³€ì¡°ìœ¨ì´ í¬ê²Œ í•˜ë½í•œ ì¥ë¹„ê°€ {fading_count}ê±´ ë°œê²¬ë¨" if fading_count > 0 else "í˜ì´ë”© ê´€ë ¨ í‚¤ì›Œë“œ ë°œê²¬",
                    "MW ì¥ë¹„ ìì²´ ê²½ë³´ ì—†ì´ ì—°ê²°ëœ ì¥ë¹„ì—ì„œ ê²½ë³´ ë°œìƒ"
                ],
                "íŒ¨í„´_ê·¼ê±°": "MWëŠ” í˜ì´ë”© ë°œìƒ ì‹œ ê²½ë³´ê°€ ì—†ìœ¼ë©° ì—°ê²°ëœ ì¥ë¹„ì—ì„œë§Œ ê²½ë³´ ë°œìƒ"
            }

    # 5.3 í•œì „ ì •ì „ ì¶”ë¡ 
    if power_outage_count > 0 or "ë°°í„°ë¦¬ ëª¨ë“œ" in cleaned_query or "UPS" in cleaned_query:
        return {
            "ì¥ì• ì ": "í•œì „ ì •ì „",
            "ì¥ì• ë¶„ì•¼": "ì „ì›",
            "ì‹ ë¢°ë„": 74.2,  # ì†Œìˆ˜ì ìœ¼ë¡œ ë³€ê²½
            "ê·¼ê±°": [
                f"ë°°í„°ë¦¬ ëª¨ë“œë¡œ ìš´ìš© ì¤‘ì¸ MW ì¥ë¹„ê°€ {power_outage_count}ê±´ ë°œê²¬ë¨" if power_outage_count > 0 else "ì „ì› ê´€ë ¨ í‚¤ì›Œë“œ ë°œê²¬",
                "ì „ì› ì¥ì•  ì‹œ ì¥ë¹„ë“¤ì´ ë°°í„°ë¦¬ ëª¨ë“œë¡œ ë™ì‘"
            ],
            "íŒ¨í„´_ê·¼ê±°": "MW ì¥ë¹„ê°€ ë°°í„°ë¦¬ ëª¨ë“œë¡œ ìš´ì˜ë˜ë©´ í•œì „ ì •ì „ì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŒ"
        }

    # 6. ë¶„ì•¼ë³„ íŠ¹í™” íŒ¨í„´ í™•ì¸ (constants.pyì˜ SPECIALIZED_FAULT_PATTERNS ì‚¬ìš©)
    specialized_pattern_result = check_specialized_patterns(
        query, field_equipment_counts, cleaned_query)
    if specialized_pattern_result:
        # ê¸°ì¡´ ì‹ ë¢°ë„ì— ì„ì˜ì˜ ì†Œìˆ˜ì  ê°’ ì¶”ê°€
        specialized_pattern_result["ì‹ ë¢°ë„"] = specialized_pattern_result.get(
            "ì‹ ë¢°ë„", 0) + 0.3
        return specialized_pattern_result

    # 7. ì¥ë¹„ ê³„ì¸µ êµ¬ì¡° ê¸°ë°˜ ì¶”ë¡ 
    if equipment_analysis["potential_fault_points"]:
        # ê°€ì¥ ë§ì€ í•˜ìœ„ ì¥ë¹„ê°€ ì–¸ê¸‰ëœ ìƒìœ„ ì¥ë¹„ë¥¼ ì¥ì• ì ìœ¼ë¡œ ì¶”ë¡ 
        best_candidate = max(
            equipment_analysis["potential_fault_points"], key=lambda x: x["í•˜ìœ„ì¥ë¹„ìˆ˜"])

        # ì¥ë¹„ê°€ ì†í•œ ë¶„ì•¼ í™•ì¸
        equipment_field = "ê¸°íƒ€"
        for field, keywords in FIELD_KEYWORDS.items():
            if best_candidate["ì¥ë¹„"] in keywords:
                equipment_field = field
                break

        return {
            "ì¥ì• ì ": f"{best_candidate['ì¥ë¹„']} ì¥ë¹„ ë¶ˆëŸ‰",
            "ì¥ì• ë¶„ì•¼": equipment_field,
            "ì‹ ë¢°ë„": 69.5,  # ì†Œìˆ˜ì ìœ¼ë¡œ ë³€ê²½
            "ê·¼ê±°": [
                f"{best_candidate['ì¥ë¹„']} ë° í•˜ìœ„ ì¥ë¹„({', '.join(best_candidate['ì–¸ê¸‰ëœí•˜ìœ„ì¥ë¹„'])})ì—ì„œ ë‹¤ìˆ˜ ê²½ë³´ ë°œìƒ",
                f"ìƒìœ„ ì¥ë¹„ì¸ {best_candidate['ì¥ë¹„']}ì˜ ì¥ì• ê°€ í•˜ìœ„ ì¥ë¹„ì— ì˜í–¥ì„ ë¯¸ì¹¨"
            ],
            "íŒ¨í„´_ê·¼ê±°": "ìƒìœ„ ì¥ë¹„ì— ì—°ê²°ëœ í•˜ìœ„ ì¥ë¹„ë“¤ì˜ ê²½ë³´ê°€ ë‹¤ìˆ˜ ë°œìƒ",
            "ì¥ë¹„_ê³„ì¸µ": explain_equipment_hierarchy(query)
        }

    # 8. ìœ ì‚¬ ì‚¬ë¡€ ì°¸ì¡° (í™•ì‹¤í•œ íŒ¨í„´ì´ ì—†ëŠ” ê²½ìš°)
    if top_results and len(top_results) > 0:
        metadata = top_results[0].get("metadata", {})

        if metadata.get("ì¥ì• ì ", "") and metadata.get("ì¥ì• ë¶„ì•¼", ""):
            similarity = top_results[0].get("similarity", 0)
            # ìµœëŒ€ 54.7%ì˜ ì‹ ë¢°ë„ (ì†Œìˆ˜ì ìœ¼ë¡œ ë³€ê²½)
            confidence = min(54.7, similarity * 0.6)
            return {
                "ì¥ì• ì ": metadata.get("ì¥ì• ì ", ""),
                "ì¥ì• ë¶„ì•¼": metadata.get("ì¥ì• ë¶„ì•¼", ""),
                "ì‹ ë¢°ë„": round(confidence, 1),
                "ê·¼ê±°": [
                    f"ìœ ì‚¬ ì‚¬ë¡€(#{metadata.get('ì¥ì• ë²ˆí˜¸')})ì™€ì˜ ê²½ë³´ íŒ¨í„´ ìœ ì‚¬ì„±",
                    f"ìœ ì‚¬ë„ {similarity:.1f}% ê¸°ë°˜ ì¶”ë¡ "
                ],
                "íŒ¨í„´_ê·¼ê±°": "ìœ ì‚¬ ì‚¬ë¡€ ì°¸ì¡° (ëª…í™•í•œ íŒ¨í„´ ì—†ìŒ)"
            }

    # 9. ê¸°ë³¸ ì‘ë‹µ (ì¶©ë¶„í•œ ì •ë³´ ì—†ìŒ)
    highest_field = equipment_analysis["most_mentioned_field"]
    return {
        "ì¥ì• ì ": "ì •ë³´ ë¶€ì¡±ìœ¼ë¡œ íŒë‹¨ ì–´ë µìŠµë‹ˆë‹¤.",
        "ì¥ì• ë¶„ì•¼": highest_field if highest_field != "ê¸°íƒ€" else "ì•Œ ìˆ˜ ì—†ìŒ",
        "ì‹ ë¢°ë„": 19.4,  # ì†Œìˆ˜ì ìœ¼ë¡œ ë³€ê²½
        "ê·¼ê±°": [
            "ì¶©ë¶„í•œ ê²½ë³´ íŒ¨í„´ì´ ì—†ê±°ë‚˜ ëª…í™•í•œ ì¥ì•  ì§•í›„ ë¶€ì¡±",
            "ë” ë§ì€ ì •ë³´ê°€ í•„ìš”í•©ë‹ˆë‹¤"
        ],
        "íŒ¨í„´_ê·¼ê±°": "ëª…í™•í•œ íŒ¨í„´ ì—†ìŒ"
    }


def predict_fault_real_cases(top_results):
    """ì¥ì• ì  ì¶”ë¡  2: ìœ ì‚¬ ì‚¬ë¡€ ê¸°ë°˜ ì¥ì• ì  ì˜ˆì¸¡"""
    if not top_results:
        return {"ì¥ì• ì ": "ì•Œ ìˆ˜ ì—†ìŒ", "ì‹ ë¢°ë„": 0, "ê·¼ê±°": "ìœ ì‚¬ ì‚¬ë¡€ê°€ ì—†ìŠµë‹ˆë‹¤."}

    # ê°€ì¤‘ì¹˜ ì„¤ì • (ìœ ì‚¬ë„ê°€ ë†’ì„ìˆ˜ë¡ ê°€ì¤‘ì¹˜ë„ ë†’ê²Œ)
    weights = []
    for i, result in enumerate(top_results):
        similarity = result.get("similarity", 0)
        # ìœ ì‚¬ë„ì— ë”°ë¥¸ ê°€ì¤‘ì¹˜ ì„¤ì • (ë¡œê·¸ ìŠ¤ì¼€ì¼ë¡œ ì°¨ì´ ì¤„ì´ê¸°)
        weight = similarity * (1.0 - (i * 0.1))  # ìˆœìœ„ì— ë”°ë¼ ê°€ì¤‘ì¹˜ ê°ì†Œ
        weights.append(weight)

    # ì •ê·œí™”
    total_weight = sum(weights)
    if total_weight > 0:
        weights = [w / total_weight for w in weights]

    # ì¥ì• ì  ë° ì¥ì• ë¶„ì•¼ ì¶”ì¶œ ë° ì ìˆ˜ ê³„ì‚°
    fault_points = {}
    fault_fields = {}

    for i, result in enumerate(top_results):
        metadata = result.get("metadata", {})

        # ì¥ì• ì  ê°€ì¤‘ì¹˜ ê³„ì‚°
        fault_point = metadata.get("ì¥ì• ì ", "ì•Œ ìˆ˜ ì—†ìŒ")
        if fault_point and fault_point != "N/A":
            if fault_point in fault_points:
                fault_points[fault_point] += weights[i]
            else:
                fault_points[fault_point] = weights[i]

        # ì¥ì• ë¶„ì•¼ ê°€ì¤‘ì¹˜ ê³„ì‚°
        fault_field = metadata.get("ì¥ì• ë¶„ì•¼", "ê¸°íƒ€")
        if fault_field:
            if fault_field in fault_fields:
                fault_fields[fault_field] += weights[i]
            else:
                fault_fields[fault_field] = weights[i]

    # ìµœê³  ì ìˆ˜ì˜ ì¥ì• ì  ë° ì¥ì• ë¶„ì•¼ ì„ íƒ
    best_fault_point = max(fault_points.items(
    ), key=lambda x: x[1]) if fault_points else ("ì•Œ ìˆ˜ ì—†ìŒ", 0)
    best_fault_field = max(fault_fields.items(),
                           key=lambda x: x[1]) if fault_fields else ("ê¸°íƒ€", 0)

    # ì‹ ë¢°ë„ ê³„ì‚° (ê°€ì¤‘ì¹˜ ê¸°ë°˜)
    confidence = best_fault_point[1] * 100  # í¼ì„¼íŠ¸ë¡œ ë³€í™˜

    # ê·¼ê±° ìƒì„±
    evidence = []
    for result in top_results:
        metadata = result.get("metadata", {})
        if metadata.get("ì¥ì• ì ") == best_fault_point[0]:
            # ê·¼ê±°ê°€ ë˜ëŠ” ì‚¬ë¡€ ì •ë³´ ì¶”ê°€
            evidence.append(
                f"ì¥ì• ë²ˆí˜¸ #{metadata.get('ì¥ì• ë²ˆí˜¸')}: {metadata.get('ì¥ì• ëª…')} (ìœ ì‚¬ë„: {result.get('similarity'):.1f}%)")

    # ì¡°ì¹˜ë‚´ì—­ ì¶”ì¶œ (ìµœìƒìœ„ ìœ ì‚¬ ì‚¬ë¡€ì—ì„œ)
    top_actions = []
    for result in top_results:
        metadata = result.get("metadata", {})
        if metadata.get("ì¥ì• ì ") == best_fault_point[0] and metadata.get("ì¡°ì¹˜ë‚´ì—­"):
            action = metadata.get("ì¡°ì¹˜ë‚´ì—­").strip()
            if action:
                top_actions.append(action)

    return {
        "ì¥ì• ì ": best_fault_point[0],
        "ì¥ì• ë¶„ì•¼": best_fault_field[0],
        "ì‹ ë¢°ë„": min(100, max(0, round(confidence, 1))),
        "ê·¼ê±°": evidence[:3],  # ìµœëŒ€ 3ê°œ ê·¼ê±° í‘œì‹œ
        "ì¡°ì¹˜ë‚´ì—­": top_actions[:1]  # ìµœìƒìœ„ ì¡°ì¹˜ë‚´ì—­ 1ê°œë§Œ í‘œì‹œ
    }


def analyze_field_equipment_mentions(query):
    """ì¿¼ë¦¬ì—ì„œ ë¶„ì•¼ë³„ ì¥ë¹„ ì–¸ê¸‰ ë¹ˆë„ ë¶„ì„"""
    field_counts = {
        "IP": 0,
        "ì „ì†¡": 0,
        "êµí™˜": 0,
        "MW": 0,
        "ì„ ë¡œ": 0,
        "ë¬´ì„ ": 0,
        "ì „ì›": 0
    }

    query_lower = query.lower()

    # ë¶„ì•¼ë³„ í‚¤ì›Œë“œ ì¹´ìš´íŒ…
    for field, keywords in FIELD_KEYWORDS.items():
        for keyword in keywords:
            if keyword.lower() in query_lower:
                field_counts[field] += 1

    return field_counts


async def generate_brief_async(query, top_results, external_factors, fault_point_1, fault_point_2):
    """ì¿¼ë¦¬ì™€ ìœ ì‚¬ë„ ë†’ì€ ì¥ì•  ì‚¬ë¡€ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì „ë¬¸ì ì¸ ì¢…í•© ì˜ê²¬ì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜ (ë¹„ë™ê¸° ë²„ì „)"""
    if not top_results:
        return "ìœ ì‚¬í•œ ì¥ì• ì‚¬ë¡€ê°€ ì—†ì–´ ì¢…í•© ì˜ê²¬ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    # 1. ê²½ë³´ ë©”ì‹œì§€ ì •ì œ
    cleaned_query = clean_alert_message(query)
    top_similarity = top_results[0].get("similarity", 0)

    # 2. ë©”íƒ€ë°ì´í„° ê¸°ë°˜ ì •ë³´ ìˆ˜ì§‘
    fields, fault_points, fault_names, actions = [], [], [], []

    for result in top_results:
        metadata = result.get("metadata", {})
        field = metadata.get("ì¥ì• ë¶„ì•¼", "").strip()
        fault_point = metadata.get("ì¥ì• ì ", "").strip()
        fault_name = metadata.get("ì¥ì• ëª…", "").strip()
        action = metadata.get("ì¡°ì¹˜ë‚´ì—­", "").strip()

        if field:
            fields.append(field)
        if fault_point and fault_point != "N/A":
            fault_points.append(fault_point)
        if fault_name:
            fault_names.append(fault_name)
        if action and len(action) > 10:
            actions.append(action)

    # 3. ê°€ì¥ ë§ì´ ì–¸ê¸‰ëœ ì¥ì•  ë¶„ì•¼ ì¶”ì¶œ
    field_counter = {}
    for field in fields:
        field_counter[field] = field_counter.get(field, 0) + 1

    main_field = "ì•Œ ìˆ˜ ì—†ìŒ"
    if field_counter:
        main_field = max(field_counter.items(), key=lambda x: x[1])[0]

    # 4. íŒ¨í„´ ë¶„ì„ ë° ìƒê´€ê´€ê³„ ë„ì¶œ (ë³‘ë ¬ ì²˜ë¦¬)
    # ë¹„ë™ê¸° í•¨ìˆ˜ë¡œ ë³€í™˜
    async def analyze_patterns_async():
        return analyze_alert_patterns(cleaned_query)

    async def analyze_correlation_async():
        return analyze_fault_alert_correlation(top_results)

    # ë‘ ì‘ì—…ì„ ë™ì‹œì— ì‹¤í–‰
    alert_patterns, correlation = await asyncio.gather(
        analyze_patterns_async(),
        analyze_correlation_async()
    )

    # 5. ì¥ì• ì  ì¶”ë¡  ì •ë³´ í†µí•©
    # ë‘ ì¶”ë¡  ê²°ê³¼ì˜ ì‹ ë¢°ë„ ë¹„êµ (íŒ¨í„´ ê¸°ë°˜ì€ fault_point_2, ì‚¬ë¡€ ê¸°ë°˜ì€ fault_point_1)
    pattern_confidence = fault_point_2.get("ì‹ ë¢°ë„", 0)
    similar_case_confidence = fault_point_1.get("ì‹ ë¢°ë„", 0)

    # íŒ¨í„´ ê¸°ë°˜ê³¼ ìœ ì‚¬ ì‚¬ë¡€ ê¸°ë°˜ ì¶”ë¡ ì„ ê°ê° ë³€ìˆ˜ë¡œ ì €ì¥
    pattern_fault_point = fault_point_2.get("ì¥ì• ì ", "")
    pattern_fault_field = fault_point_2.get("ì¥ì• ë¶„ì•¼", "")
    pattern_evidence = fault_point_2.get("ê·¼ê±°", [])

    similar_fault_point = fault_point_1.get("ì¥ì• ì ", "")
    similar_fault_field = fault_point_1.get("ì¥ì• ë¶„ì•¼", "")
    similar_evidence = fault_point_1.get("ê·¼ê±°", [])

    # ì‹ ë¢°ë„ê°€ ë” ë†’ì€ ìª½ì„ ì„ íƒ
    higher_confidence_field = pattern_fault_field if pattern_confidence >= similar_case_confidence else similar_fault_field
    higher_confidence_value = max(pattern_confidence, similar_case_confidence)

    # ì¡°ì¹˜ë‚´ì—­ í†µí•©
    main_action = ""
    if fault_point_1.get("ì¡°ì¹˜ë‚´ì—­") and isinstance(fault_point_1.get("ì¡°ì¹˜ë‚´ì—­"), list):
        main_action = fault_point_1.get("ì¡°ì¹˜ë‚´ì—­")[0]
    elif actions:
        main_action = actions[0]

    # 6. ì™¸ë¶€ ìš”ì¸ ì •ë³´ ìƒì„±
    external_factors_info = generate_external_factors_info(external_factors)

    # 7. ìœ ì‚¬ë„ êµ¬ê°„ë³„ ì¢…í•© ì˜ê²¬ ìƒì„± (ìƒˆë¡œìš´ í˜•ì‹ìœ¼ë¡œ)
    if top_similarity < 40.0:
        return generate_low_similarity_opinion(
            cleaned_query, higher_confidence_field, higher_confidence_value,
            pattern_fault_point, pattern_confidence, pattern_evidence,
            similar_fault_point, similar_case_confidence, similar_evidence,
            external_factors_info, top_similarity
        )
    elif top_similarity < 60.0:
        return generate_medium_similarity_opinion(
            higher_confidence_field, higher_confidence_value,
            pattern_fault_point, pattern_confidence, pattern_evidence,
            similar_fault_point, similar_case_confidence, similar_evidence,
            correlation, external_factors_info, main_action, top_similarity
        )
    else:
        return generate_high_similarity_opinion(
            higher_confidence_field, higher_confidence_value,
            pattern_fault_point, pattern_confidence, pattern_evidence,
            similar_fault_point, similar_case_confidence, similar_evidence,
            correlation, main_action, external_factors_info, top_similarity
        )

"""
ê²½ë³´ ìœ ì‚¬ë„ì— ë”°ë¥¸ ì¥ì•  ë¶„ì„ ì˜ê²¬ì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜
"""


def format_evidence_text(evidence_list):
    """ì¦ê±° ëª©ë¡ì„ í¬ë§·íŒ… """
    if not evidence_list or len(evidence_list) == 0:
        return ""

    return "\n".join([f"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- {evidence}" for evidence in evidence_list])


def build_inference_section(
    pattern_fault_point,
    pattern_confidence,
    pattern_evidence_text,
    similar_fault_point,
    similar_case_confidence,
    similar_evidence_text
):
    """ì¶”ë¡  ê²°ê³¼ ì„¹ì…˜ ìƒì„±"""
    # ì¶”ë¡  ê²°ê³¼ê°€ ë‹¤ë¥¸ì§€ í™•ì¸
    different_results = pattern_fault_point != similar_fault_point
    different_results_text = "\nâ€» ì¶”ë¡  1ê³¼ ì¶”ë¡  2ì˜ ê²°ê³¼ê°€ ë‹¤ë¦…ë‹ˆë‹¤. ë‘ ê°€ì§€ ê°€ëŠ¥ì„±ì„ ëª¨ë‘ ê²€í† í•˜ì„¸ìš”." if different_results else ""

    return f"""
ğŸ” <span class="inference-title"><b>ì¥ì• ì  ì¶”ë¡  1 (íŒ¨í„´ ê¸°ë°˜): </b></span> <span class="fault-result" style="color: red;"><b>{pattern_fault_point}</b></span>
&nbsp&nbsp&nbspâ€¢ <b>ì‹ ë¢°ë„: {pattern_confidence:.1f}%</b>
&nbsp&nbsp&nbspâ€¢ <b>íŒë‹¨ê¸°ì¤€:</b>
{pattern_evidence_text}

ğŸ” <span class="inference-title"><b>ì¥ì• ì  ì¶”ë¡  2 (ìœ ì‚¬ ì¥ì• ì‚¬ë¡€ ê¸°ë°˜):</b></span> <span class="fault-result" style="color: red;"><b>{similar_fault_point}</b></span>
&nbsp&nbsp&nbspâ€¢ <b>ì‹ ë¢°ë„: {similar_case_confidence:.1f}%</b>
&nbsp&nbsp&nbspâ€¢ <b>ìœ ì‚¬ì‚¬ë¡€:</b>
{similar_evidence_text}<br>{different_results_text}
"""


def build_common_header(
    external_factors_info,
    higher_confidence_index,
    higher_confidence_field,
    higher_confidence_value
):
    """ì˜ê²¬ì˜ ê³µí†µ í—¤ë” ë¶€ë¶„ ìƒì„±"""
    pattern_based = higher_confidence_index == 1
    return f"""
ì™¸ë¶€ ìš”ì¸ìœ¼ë¡œëŠ”, {external_factors_info}

í˜„ì¬ ê²½ë³´ëŠ” <span class="header-inference-mention">ì¥ì• ì  ì¶”ë¡  {higher_confidence_index}({"íŒ¨í„´ ê¸°ë°˜" if pattern_based else "ìœ ì‚¬ ì¥ì• ì‚¬ë¡€ ê¸°ë°˜"})</span>ì˜ ê²°ê³¼, <b>{higher_confidence_field} ë¶„ì•¼</b>ì˜ ì¥ì• ì ê³¼ ìœ ì‚¬í•˜ë©° <b>ì‹ ë¢°ë„ {higher_confidence_value:.1f}%</b> ìˆ˜ì¤€ì…ë‹ˆë‹¤.
"""


def build_low_similarity_footer(cleaned_query, top_similarity):
    return f"""
í˜„ì¬ ë°œìƒí•œ ê²½ë³´ëŠ” <b>ì¥ì• ì‚¬ë¡€ì™€ ìœ ì‚¬ë„({top_similarity:.1f}%)ê°€ ë‚®ì•„</b> ì •í™•í•œ ì¥ì• ì  íŒë‹¨ì€ ì–´ë µìŠµë‹ˆë‹¤. 
ì¶”ê°€ì ì¸ ê²½ë³´ ë‚´ì—­ì´ë‚˜ ì¥ì•  ì¦ìƒ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.
ì…ë ¥ëœ ê²½ë³´ ë‚´ì—­({cleaned_query})ìœ¼ë¡œ ì¶”ì •ëœ ê²°ê³¼ë¡œëŠ” ëª…í™•í•œ íŒ¨í„´ì„ ì°¾ê¸° ì–´ë µìŠµë‹ˆë‹¤.
ê´€ë ¨ ë¶„ì•¼ë³„(MW, IP, ì „ì†¡, êµí™˜, ë¬´ì„ , ì „ì›) ì¥ë¹„ë“¤ì—ì„œ ë°œìƒí•œ ê²½ë³´(ë¡œê·¸) ë‚´ì—­ê³¼ ì¥ì•  ì¦ìƒì„ ì¶”ê°€ë¡œ ì…ë ¥í•´ ì£¼ì‹œê¸° ë°”ëë‹ˆë‹¤.
"""


def build_medium_similarity_footer(higher_confidence_field, correlation, main_action):
    """ì¤‘ê°„ ìœ ì‚¬ë„ ì¼€ì´ìŠ¤ë¥¼ ìœ„í•œ í‘¸í„° ìƒì„±"""
    action_text = f"\ní•´ë‹¹ ìœ í˜•ì˜ ì¥ì• ëŠ” ì¼ë°˜ì ìœ¼ë¡œ <b>{main_action}</b>ìœ¼ë¡œ ë³µêµ¬ë˜ì—ˆìŠµë‹ˆë‹¤." if main_action else ""

    return f"""
{correlation}

ì´ ìƒí™©ì—ì„œëŠ” MW/IP/ì „ì†¡/êµí™˜/ë¬´ì„ /ì„ ë¡œ/ì „ì›ì—ì„œë„ ì¶”ê°€ ê²½ë³´ê°€ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ìœ ì‚¬ ì¥ì• ì‚¬ë¡€ë¥¼ ì°¸ê³ í•˜ì—¬, <b>{higher_confidence_field} ê´€ë ¨ ì¥ë¹„ë¥¼ ìš°ì„ ì ìœ¼ë¡œ ì ê²€</b>í•´ë³´ì‹œê¸° ë°”ëë‹ˆë‹¤.{action_text}
"""


def build_high_similarity_footer(higher_confidence_field, correlation, main_action):
    """ë†’ì€ ìœ ì‚¬ë„ ì¼€ì´ìŠ¤ë¥¼ ìœ„í•œ í‘¸í„° ìƒì„±"""
    action_text = main_action[:200] + "..." if main_action and len(
        main_action) > 200 else main_action if main_action else "ê¸°ë¡ëœ ì¡°ì¹˜ë‚´ì—­ ì—†ìŒ"

    return f"""
{correlation}

ì´ëŸ¬í•œ ìƒí™©ì—ì„œëŠ” MW/IP/ì „ì†¡/êµí™˜/ë¬´ì„ /ì„ ë¡œ/ì „ì›ì—ì„œë„ ì¶”ê°€ ê²½ë³´ê°€ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
í•´ë‹¹ ìœ ì‚¬ ì¥ì• ì‚¬ë¡€ë¥¼ ì°¸ê³ í•˜ì—¬, <b>{higher_confidence_field} ê´€ë ¨ ì¥ë¹„ë¥¼ ìš°ì„ ì ìœ¼ë¡œ ì ê²€</b>í•´ë³´ì‹œê¸° ë°”ëë‹ˆë‹¤.

<br> ìœ ì‚¬ ì‚¬ë¡€ì˜ ì¡°ì¹˜ë‚´ì—­ì„ ì°¸ê³ í•˜ë©´, <b>{action_text}</b>ìœ¼ë¡œ ë³µêµ¬ë˜ì—ˆìŠµë‹ˆë‹¤.
"""


def determine_higher_confidence(pattern_confidence, similar_case_confidence):
    """ë” ë†’ì€ ì‹ ë¢°ë„ë¥¼ ê°€ì§„ ì¶”ë¡  ê²°ê³¼ ê²°ì •"""
    if pattern_confidence >= similar_case_confidence:
        return 1, pattern_confidence
    else:
        return 2, similar_case_confidence


def generate_low_similarity_opinion(
    cleaned_query, higher_confidence_field, higher_confidence_value,
    pattern_fault_point, pattern_confidence, pattern_evidence,
    similar_fault_point, similar_case_confidence, similar_evidence,
    external_factors_info, top_similarity
):
    """ë‚®ì€ ìœ ì‚¬ë„ (40% ë¯¸ë§Œ)ì˜ ê²½ìš° ìƒì„±í•  ì˜ê²¬"""
    # ë” ë†’ì€ ì‹ ë¢°ë„ë¥¼ ê°€ì§„ ì¶”ë¡  ê²°ê³¼ í™•ì¸
    higher_confidence_index, _ = determine_higher_confidence(
        pattern_confidence, similar_case_confidence)

    # ì¦ê±° í…ìŠ¤íŠ¸ í¬ë§·íŒ…
    pattern_evidence_text = format_evidence_text(pattern_evidence)
    similar_evidence_text = format_evidence_text(similar_evidence)

    # ì˜ê²¬ êµ¬ì„±
    header = build_common_header(
        external_factors_info,
        higher_confidence_index,
        higher_confidence_field,
        higher_confidence_value
    )

    inference_section = build_inference_section(
        pattern_fault_point,
        pattern_confidence,
        pattern_evidence_text,
        similar_fault_point,
        similar_case_confidence,
        similar_evidence_text
    )

    footer = build_low_similarity_footer(cleaned_query, top_similarity)

    return header + inference_section + footer


def generate_medium_similarity_opinion(
    higher_confidence_field, higher_confidence_value,
    pattern_fault_point, pattern_confidence, pattern_evidence,
    similar_fault_point, similar_case_confidence, similar_evidence,
    correlation, external_factors_info, main_action, top_similarity
):
    """ì¤‘ê°„ ìœ ì‚¬ë„ (40-60%)ì˜ ê²½ìš° ìƒì„±í•  ì˜ê²¬"""
    # ë” ë†’ì€ ì‹ ë¢°ë„ë¥¼ ê°€ì§„ ì¶”ë¡  ê²°ê³¼ í™•ì¸
    higher_confidence_index, _ = determine_higher_confidence(
        pattern_confidence, similar_case_confidence)

    # ì¦ê±° í…ìŠ¤íŠ¸ í¬ë§·íŒ…
    pattern_evidence_text = format_evidence_text(pattern_evidence)
    similar_evidence_text = format_evidence_text(similar_evidence)

    # ì˜ê²¬ êµ¬ì„±
    header = build_common_header(
        external_factors_info,
        higher_confidence_index,
        higher_confidence_field,
        higher_confidence_value
    )

    inference_section = build_inference_section(
        pattern_fault_point,
        pattern_confidence,
        pattern_evidence_text,
        similar_fault_point,
        similar_case_confidence,
        similar_evidence_text
    )

    footer = build_medium_similarity_footer(
        higher_confidence_field, correlation, main_action)

    return header + inference_section + footer


def generate_high_similarity_opinion(
    higher_confidence_field, higher_confidence_value,
    pattern_fault_point, pattern_confidence, pattern_evidence,
    similar_fault_point, similar_case_confidence, similar_evidence,
    correlation, main_action, external_factors_info, top_similarity
):
    """ë†’ì€ ìœ ì‚¬ë„ (60% ì´ìƒ)ì˜ ê²½ìš° ìƒì„±í•  ì˜ê²¬"""
    # ë” ë†’ì€ ì‹ ë¢°ë„ë¥¼ ê°€ì§„ ì¶”ë¡  ê²°ê³¼ í™•ì¸
    higher_confidence_index, _ = determine_higher_confidence(
        pattern_confidence, similar_case_confidence)

    # ì¦ê±° í…ìŠ¤íŠ¸ í¬ë§·íŒ…
    pattern_evidence_text = format_evidence_text(pattern_evidence)
    similar_evidence_text = format_evidence_text(similar_evidence)

    # ì˜ê²¬ êµ¬ì„±
    header = build_common_header(
        external_factors_info,
        higher_confidence_index,
        higher_confidence_field,
        higher_confidence_value
    )

    inference_section = build_inference_section(
        pattern_fault_point,
        pattern_confidence,
        pattern_evidence_text,
        similar_fault_point,
        similar_case_confidence,
        similar_evidence_text
    )

    footer = build_high_similarity_footer(
        higher_confidence_field, correlation, main_action)

    return header + inference_section + footer


def generate_external_factors_info(external_factors):
    """ì™¸ë¶€ ìš”ì¸ ì •ë³´ë¥¼ ë¬¸ìì—´ë¡œ ìƒì„±"""
    fading_count = external_factors.get("fading_count", 0)
    power_outage_count = external_factors.get("power_outage_count", 0)
    cable_damage_count = external_factors.get("cable_damage_count", 0)

    # ëª¨ë“  ì™¸ë¶€ ìš”ì¸ì´ 0ì´ë©´ í†µí•© ë©”ì‹œì§€
    if fading_count == 0 and power_outage_count == 0 and cable_damage_count == 0:
        return """MW ì „íŒŒ <b>í˜ì´ë”© ì˜í–¥ì€ ì—†ìœ¼ë©°, í•œì „ ì •ì „ ì˜í–¥ë„ ì—†ê³ , ì„ ë¡œ ì¥ì• ë„ ì—†ëŠ”</b> ê²ƒìœ¼ë¡œ í™•ì¸ë©ë‹ˆë‹¤.
ë‹¤ë¥¸ ë¶„ì•¼ë³„/ì¥ë¹„ë³„ ê²½ë³´í˜„í™©ì„ ì¶”ê°€ë¡œ í™•ì¸/ë¶„ì„í•  í•„ìš”ê°€ ìˆìŠµë‹ˆë‹¤."""

    # ê° ìš”ì¸ë³„ ë©”ì‹œì§€ ìƒì„±
    external_factors_info = []

    if fading_count > 0:
        external_factors_info.append(
            f"- MW ì¥ë¹„ ì¤‘ ë³€ì¡°ìœ¨ì´ í¬ê²Œ í•˜ë½í•œ ì¥ë¹„ê°€({fading_count}ê±´) ìˆê¸° ë•Œë¬¸ì— <b>ì „íŒŒ í˜ì´ë”©</b>ì´ ë¬¸ì œì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

    if power_outage_count > 0:
        external_factors_info.append(
            f"- ì¼ë¶€ MW ì¥ë¹„ê°€({power_outage_count}ê±´) ë°°í„°ë¦¬ ëª¨ë“œë¡œ ìš´ìš©ë˜ê³  ìˆì–´ <b>í•œì „ ì •ì „</b>ì´ ë¬¸ì œì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

    if cable_damage_count > 0:
        external_factors_info.append(
            f"- ë¯¸ë³µêµ¬ëœ <b>ì„ ë¡œ ì¥ì• </b>ê°€ {cable_damage_count}ê±´ì´ ìˆì–´ ì„ ë¡œê°€ ë¬¸ì œì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

    return "\n".join(external_factors_info)


def analyze_alert_patterns(cleaned_query):
    """ê²½ë³´ íŒ¨í„´ì„ ë¶„ì„í•˜ì—¬ ì£¼ìš” íŠ¹ì§•ì„ ì¶”ì¶œ"""
    # ê²½ë³´ íŒ¨í„´ì˜ ì¼ë°˜ì  íŠ¹ì„± ë¶„ì„
    alert_keywords = []

    # ì£¼ìš” ì¥ë¹„ í‚¤ì›Œë“œ í™•ì¸ - ë‹¨ìˆœ í¬í•¨ ì—¬ë¶€ í™•ì¸ìœ¼ë¡œ ìµœì í™”
    cleaned_query_lower = cleaned_query.lower()
    for keyword in EQUIPMENT_KEYWORDS:
        if keyword.lower() in cleaned_query_lower:
            alert_keywords.append(keyword)
            break

    # ì£¼ìš” ê²½ë³´ ìœ í˜• í‚¤ì›Œë“œ í™•ì¸
    for keyword in ALERT_TYPE_KEYWORDS:
        if keyword.lower() in cleaned_query_lower:
            alert_keywords.append(keyword)
            break

    # í‚¤ì›Œë“œ ì¡°í•©ìœ¼ë¡œ ì¼ë°˜í™”ëœ ê²½ë³´ íŒ¨í„´ ì„¤ëª… ìƒì„±
    if alert_keywords:
        return f"í˜„ì¬ ê²½ë³´ëŠ” {', '.join(alert_keywords)} ê´€ë ¨ íŠ¹ì„±ì„ ë³´ì´ê³  ìˆìŠµë‹ˆë‹¤."
    else:
        return "í˜„ì¬ ê²½ë³´ íŒ¨í„´ì—ì„œ íŠ¹ì • ì¥ë¹„ë‚˜ ì¥ì•  ìœ í˜•ì˜ ëª…í™•í•œ íŠ¹ì„±ì„ íŒŒì•…í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤."


def analyze_fault_alert_correlation(top_results):
    """ì¥ì• ì™€ ê²½ë³´ì˜ ìƒê´€ê´€ê³„ë¥¼ ì¼ë°˜í™”ëœ ë°©ì‹ìœ¼ë¡œ ë¶„ì„"""
    if not top_results:
        return "ê´€ë ¨ ì¥ì•  ì‚¬ë¡€ê°€ ë¶€ì¡±í•˜ì—¬ ìƒê´€ê´€ê³„ ë¶„ì„ì´ ì–´ë µìŠµë‹ˆë‹¤."

    # ìƒìœ„ ê²°ê³¼ë“¤ì˜ ì¥ì• ë¶„ì•¼ ë° ì¥ì• ì  í™•ì¸
    fields = set()
    fault_points = set()

    for result in top_results:
        metadata = result.get("metadata", {})
        field = metadata.get("ì¥ì• ë¶„ì•¼", "")
        fault_point = metadata.get("ì¥ì• ì ", "")

        if field:
            fields.add(field)
        if fault_point:
            fault_points.add(fault_point)

    # ì¥ì• ë¶„ì•¼ë³„ íŠ¹ì„± ë¶„ì„ (ë°ì´í„° ê¸°ë°˜)
    correlation_texts = []

    # ì¥ì• ë¶„ì•¼ ê¸°ë°˜ ì¼ë°˜ì  íŒ¨í„´ ì„¤ëª…
    if fields:
        field_text = f"ì¡°íšŒëœ ìœ ì‚¬ ì¥ì• ì‚¬ë¡€ ë¶„ì„ ê²°ê³¼, {'/'.join(fields)} ë¶„ì•¼ì—ì„œ ì£¼ë¡œ ë°œìƒí•˜ëŠ” íŒ¨í„´ì…ë‹ˆë‹¤."
        correlation_texts.append(field_text)

    # ì¥ì• ì  ê¸°ë°˜ ì¼ë°˜ì  íŒ¨í„´ ì„¤ëª…
    fault_points_list = list(fault_points)
    if fault_points_list and len(fault_points_list) <= 3:
        points_text = f"<br>ì£¼ìš” ì¥ì• ì ìœ¼ë¡œëŠ” {', '.join(fault_points_list[:3])} ë“±ì´ í™•ì¸ë©ë‹ˆë‹¤."
        correlation_texts.append(points_text)

    # ê²½ë³´ ì—°ì‡„ íš¨ê³¼ì— ëŒ€í•œ ì¼ë°˜ì  ì„¤ëª…
    general_pattern = "<br>ì´ëŸ¬í•œ ìœ í˜•ì˜ ì¥ì• ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ì—°ê²°ëœ ë‹¤ë¥¸ ì¥ë¹„ì—ì„œë„ ì—°ì‡„ì ì¸ ê²½ë³´ê°€ ë°œìƒí•˜ëŠ” íŠ¹ì§•ì´ ìˆìŠµë‹ˆë‹¤."
    correlation_texts.append(general_pattern)

    return " ".join(correlation_texts)


async def hybrid_search_async(query, collection, top_k=5):
    """ë²¡í„° ìœ ì‚¬ë„ì™€ í‚¤ì›Œë“œ/íŒ¨í„´ ë§¤ì¹­ì„ ê²°í•©í•œ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ êµ¬í˜„ (ë¹„ë™ê¸° ë²„ì „)"""
    # ìºì‹œ í‚¤ ìƒì„±
    cache_key = hash(query)
    current_time = time.time()

    # ìºì‹œì— ìˆê³  ë§Œë£Œë˜ì§€ ì•Šì•˜ìœ¼ë©´ ìºì‹œëœ ê²°ê³¼ ë°˜í™˜
    if cache_key in _vector_search_cache:
        cached_item = _vector_search_cache[cache_key]
        if current_time - cached_item['timestamp'] < _VECTOR_CACHE_EXPIRY:
            return cached_item['results'], cached_item['search_results']

    # ë²¡í„° ê²€ìƒ‰ ì‹¤í–‰
    search_results = collection.query(
        query_texts=[query],
        n_results=min(top_k * 3, 15),
        include=["documents", "metadatas", "distances"],
    )

    if not search_results.get("documents") or not search_results["documents"][0]:
        return [], search_results

    seen_fault_numbers = set()
    docs = search_results["documents"][0]
    metas = search_results["metadatas"][0]
    distances = search_results["distances"][0]

    # ìœ ì‚¬ë„ ê³„ì‚°ì„ ìœ„í•œ ë°ì´í„° êµ¬ì„±
    documents_info = []
    for doc, meta, distance in zip(docs, metas, distances):
        fault_number = meta.get("ì¥ì• ë²ˆí˜¸")
        if not fault_number or fault_number in seen_fault_numbers:
            continue
        seen_fault_numbers.add(fault_number)
        documents_info.append({
            "alerts": meta.get("ê²½ë³´í˜„í™©", ""),
            "analysis": meta.get("ì¥ì• ë¶„ì„", ""),
            "reception": meta.get("ì¥ì• ì ‘ìˆ˜ë‚´ì—­", ""),
            "metadata": meta,
            "document": doc,
            "distance": distance,
        })

    # ìœ ì‚¬ë„ ê³„ì‚° (í…ìŠ¤íŠ¸ ë§¤ì¹­ ê°•í™”)
    similarity_scores = await calculate_hybrid_similarities(query, documents_info)

    hybrid_results = []
    for i, sim_score in enumerate(similarity_scores):
        hybrid_results.append({
            "document": documents_info[i]["document"],
            "metadata": documents_info[i]["metadata"],
            "vector_distance": documents_info[i]["distance"],
            "similarity": sim_score,
            "hybrid_score": sim_score
        })

    sorted_results = sorted(
        hybrid_results, key=lambda x: x["hybrid_score"], reverse=True)[:top_k]

    # ê²°ê³¼ ìºì‹±
    _vector_search_cache[cache_key] = {
        'results': sorted_results,
        'search_results': search_results,
        'timestamp': current_time
    }

    # ìºì‹œ í¬ê¸° ê´€ë¦¬
    if len(_vector_search_cache) > _VECTOR_CACHE_SIZE:
        oldest_keys = sorted(_vector_search_cache.keys(),
                             key=lambda k: _vector_search_cache[k]['timestamp'])[:5]
        for key in oldest_keys:
            del _vector_search_cache[key]

    return sorted_results, search_results


async def calculate_hybrid_similarities(query, documents):
    """í•˜ì´ë¸Œë¦¬ë“œ ìœ ì‚¬ë„ ê³„ì‚° - ë²¡í„° ê±°ë¦¬, í…ìŠ¤íŠ¸ ë§¤ì¹­, íŒ¨í„´ ë§¤ì¹­ ê²°í•©"""
    # 1. ì •ê·œí™” ë° ì „ì²˜ë¦¬
    query_norm = normalize_text(query.lower())
    query_codes = set(extract_alert_codes_cached(query))

    # 2. ë¹„ë™ê¸° ì²˜ë¦¬ë¥¼ ìœ„í•œ ì‘ì—… ëª©ë¡ êµ¬ì„±
    tasks = []

    for doc in documents:
        # ë¹„ë™ê¸° ì²˜ë¦¬í•  ì‘ì—… ì¶”ê°€
        tasks.append(
            compute_document_similarity(
                query_norm,
                doc,
                query_codes
            )
        )

    # 3. ëª¨ë“  ìœ ì‚¬ë„ ê³„ì‚° ì‘ì—… ë³‘ë ¬ ì‹¤í–‰
    similarity_scores = await asyncio.gather(*tasks)

    return similarity_scores


async def compute_document_similarity(query_norm, doc, query_codes):
    """ë‹¨ì¼ ë¬¸ì„œì˜ ìœ ì‚¬ë„ ê³„ì‚° - ë¹„ë™ê¸° ì²˜ë¦¬"""
    # 1. ë¬¸ì„œ í•„ë“œ ì •ê·œí™”
    alert_text = normalize_text(doc.get("alerts", "").lower())
    analysis_text = normalize_text(doc.get("analysis", "").lower())
    reception_text = normalize_text(doc.get("reception", "").lower())

    # 2. ì •í™•í•œ ì¼ì¹˜ ì—¬ë¶€ í™•ì¸
    exact_match = False
    if query_norm and (query_norm == alert_text or query_norm in alert_text):
        exact_match = True

    # 3. RapidFuzz ìœ ì‚¬ë„ ê³„ì‚°
    # ê²½ë³´ë‚´ì—­, ì¥ì• ë¶„ì„, ì¥ì• ì ‘ìˆ˜ë‚´ì—­ ê°ê°ì— ëŒ€í•œ ìœ ì‚¬ë„ ê³„ì‚°
    alert_similarity = fuzz.token_set_ratio(query_norm, alert_text) / 100
    analysis_similarity = fuzz.token_set_ratio(query_norm, analysis_text) / 100
    reception_similarity = fuzz.token_set_ratio(
        query_norm, reception_text) / 100

    # 4. ê²½ë³´ ì½”ë“œ ë§¤ì¹­
    doc_codes = set(extract_alert_codes_cached(doc.get("alerts", "")))
    code_match_ratio = len(query_codes & doc_codes) / \
        len(query_codes) if query_codes else 0

    # 5. ë¶„ì•¼ í‚¤ì›Œë“œ ë§¤ì¹­
    field_match_score = calculate_field_matching(
        query_norm, alert_text + analysis_text)

    # 6. ë²¡í„° ê±°ë¦¬ ê¸°ë°˜ ì ìˆ˜ (ì—­ìˆ˜ ê´€ê³„: ê±°ë¦¬ê°€ ì‘ì„ìˆ˜ë¡ ìœ ì‚¬ë„ ë†’ìŒ)
    vector_score = max(0, 1 - doc.get("distance", 0)) * 40  # ìµœëŒ€ 40ì 

    # 7. ìµœì¢… ìœ ì‚¬ë„ ê³„ì‚°
    # ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ê²½ìš° ë†’ì€ ì ìˆ˜ ë¶€ì—¬
    if exact_match:
        return 95.0

    # ê° êµ¬ì„± ìš”ì†Œë³„ ê°€ì¤‘ì¹˜ ì¡°ì •
    alert_weight = 0.35       # ê²½ë³´ë‚´ì—­ ìœ ì‚¬ë„ ê°€ì¤‘ì¹˜
    analysis_weight = 0.25    # ì¥ì• ë¶„ì„ ìœ ì‚¬ë„ ê°€ì¤‘ì¹˜
    reception_weight = 0.15   # ì¥ì• ì ‘ìˆ˜ë‚´ì—­ ìœ ì‚¬ë„ ê°€ì¤‘ì¹˜
    code_weight = 0.15        # ê²½ë³´ì½”ë“œ ë§¤ì¹­ ê°€ì¤‘ì¹˜
    field_weight = 0.10       # ë¶„ì•¼ í‚¤ì›Œë“œ ë§¤ì¹­ ê°€ì¤‘ì¹˜

    # ê° ì»´í¬ë„ŒíŠ¸ ì ìˆ˜ ê³„ì‚°
    alert_score = alert_similarity * 100 * alert_weight
    analysis_score = analysis_similarity * 100 * analysis_weight
    reception_score = reception_similarity * 100 * reception_weight
    code_score = code_match_ratio * 100 * code_weight
    field_score = field_match_score * field_weight

    # ì´ì  ê³„ì‚° ë° ë³´ì •
    total_score = alert_score + analysis_score + reception_score + \
        code_score + field_score + (vector_score * 0.2)

    # ë†’ì€ ìœ ì‚¬ë„ ë³´ì •
    if alert_similarity > 0.9:
        total_score = max(total_score, 85)
    if alert_similarity > 0.8 and analysis_similarity > 0.7:
        total_score = max(total_score, 80)
    if code_match_ratio > 0.7:
        total_score = max(total_score, 75)

    # ìµœì¢… ì ìˆ˜ ë²”ìœ„ ì œí•œ
    return max(10, min(95, total_score))


def create_embedding_function():
    """ì„ë² ë”© í•¨ìˆ˜ ìƒì„±"""
    return embedding_functions.SentenceTransformerEmbeddingFunction(model_name=EMBEDDING_MODEL)


def get_vector_db_collection():
    """ë²¡í„°DB ì»¬ë ‰ì…˜ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜ (ì‹±ê¸€í†¤ íŒ¨í„´ ì ìš©)"""
    global _collection_instance

    if _collection_instance is not None:
        return _collection_instance, None

    try:
        # ì£¼ ê²½ë¡œ ì‹œë„
        if os.path.exists(VECTOR_DB_DIR):
            client = chromadb.PersistentClient(path=VECTOR_DB_DIR)
        # ëŒ€ì²´ ê²½ë¡œ ì‹œë„
        elif os.path.exists(VECTOR_DB_NEW_DIR):
            client = chromadb.PersistentClient(path=VECTOR_DB_NEW_DIR)
        else:
            error_msg = f"ë²¡í„°DBë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.\nì£¼ ê²½ë¡œ: {VECTOR_DB_DIR}\nëŒ€ì²´ ê²½ë¡œ: {VECTOR_DB_NEW_DIR}"
            return None, {"type": ERROR_DB_ACCESS, "message": error_msg}

        # ì„ë² ë”© í•¨ìˆ˜ ì„¤ì •
        ef = create_embedding_function()

        # ì»¬ë ‰ì…˜ ê°€ì ¸ì˜¤ê¸°
        _collection_instance = client.get_collection(
            name="nw_incidents", embedding_function=ef)

        # ì¸ë±ìŠ¤ ìµœì í™” ì‹œë„ (ì§€ì›ë˜ëŠ” ê²½ìš°)
        try:
            _collection_instance.create_index(
                index_type="hnsw",  # ëŒ€ìš©ëŸ‰ ë°ì´í„°ì— ì í•©í•œ ì¸ë±ìŠ¤
                params={"space_type": "cosine", "ef_construction": 200}
            )
        except (AttributeError, NotImplementedError):
            pass  # ì§€ì›ë˜ì§€ ì•ŠëŠ” ê²½ìš° ë¬´ì‹œ

        return _collection_instance, None

    except Exception as e:
        error_msg = f"ë²¡í„°DB ì ‘ê·¼ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        return None, {"type": ERROR_DB_ACCESS, "message": error_msg}


def build_summary_rows(top_results):
    """ê²°ê³¼ ìš”ì•½ í–‰ êµ¬ì„±"""
    return [build_result_row(r, i) for i, r in enumerate(top_results, 1)]


def build_details(top_results):
    """ìƒì„¸ ê²°ê³¼ êµ¬ì„±"""
    details = []
    for i, r in enumerate(top_results, 1):
        m = r["metadata"]
        field = m.get("ì¥ì• ë¶„ì•¼") or identify_field_from_keywords(
            m.get("ê²½ë³´í˜„í™©", ""))

        # ì‹ ë¢°ë„ ê³„ì‚° - ìœ ì‚¬ë„ì— ê¸°ë°˜í•˜ì—¬ ê³„ì‚° (ì¼ê´€ì„± ìœ ì§€)
        confidence = min(100, max(10, round(r['similarity'] * 0.9, 1)))

        details.append({
            "ìˆœìœ„": str(i),
            "ì‹ ë¢°ë„": f"{confidence:.1f}%",
            "ìœ ì‚¬ë„": f"{r['similarity']:.1f}%",
            "ë¶„ì•¼": field,
            "ì¥ì• ì ": m.get("ì¥ì• ì ", "N/A"),
            "ë°œìƒì¼ì": m.get("ë°œìƒì¼ì", "N/A"),
            "ì¥ì• ì‚¬ë¡€": f"[ì¥ì• ë²ˆí˜¸ #{m.get('ì¥ì• ë²ˆí˜¸', 'N/A')}] {m.get('ì¥ì• ëª…', 'N/A')}",
            "ì¥ì• ë¶„ì„": m.get("ì¥ì• ë¶„ì„", ""),
            "ê²½ë³´í˜„í™©": m.get("ê²½ë³´í˜„í™©", ""),
            "ì¡°ì¹˜ë‚´ì—­": m.get("ì¡°ì¹˜ë‚´ì—­", "")
        })
    return details


def build_result_row(result, i):
    """ê²°ê³¼ í–‰ êµ¬ì„± í—¬í¼ í•¨ìˆ˜"""
    metadata = result["metadata"]
    field = metadata.get("ì¥ì• ë¶„ì•¼") or identify_field_from_keywords(
        metadata.get("ê²½ë³´í˜„í™©", ""))

    # ì‹ ë¢°ë„ ê³„ì‚° - ìœ ì‚¬ë„ì— ê¸°ë°˜í•˜ì—¬ ê³„ì‚° (ì¼ê´€ì„± ìœ ì§€)
    confidence = min(100, max(10, round(result['similarity'] * 0.9, 1)))

    return {
        "ìˆœìœ„": str(i),
        "ì‹ ë¢°ë„": f"{confidence:.1f}%",
        "ìœ ì‚¬ë„": f"{result['similarity']:.1f}%",
        "ë¶„ì•¼": field,
        "ì¥ì• ì ": metadata.get("ì¥ì• ì ", "N/A"),
        "ì¥ì• ì‚¬ë¡€": metadata.get("ì¥ì• ëª…", "N/A"),
    }


# ì½”ë£¨í‹´ì„ ë™ê¸° í•¨ìˆ˜ë¡œ ë³€í™˜í•˜ëŠ” í—¬í¼ í•¨ìˆ˜
def run_coroutine_sync(coroutine_func, *args, **kwargs):
    """ì½”ë£¨í‹´ í•¨ìˆ˜ë¥¼ ë™ê¸° í•¨ìˆ˜ë¡œ ì‹¤í–‰í•˜ëŠ” í—¬í¼"""
    try:
        loop = asyncio.get_event_loop()
    except RuntimeError:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)

    return loop.run_until_complete(coroutine_func(*args, **kwargs))


# ë©”ì¸ API ì§„ì…ì 
def query(mode, query_text, user_id="default_user"):
    """ë™ê¸° APIë¥¼ ìœ„í•œ ë˜í¼ í•¨ìˆ˜"""
    return run_coroutine_sync(run_query, mode, query_text, user_id)


# APIë¥¼ ìœ„í•œ ì§ì ‘ ì‹¤í–‰ ì§€ì 
if __name__ == "__main__":
    print("ì¥ì•  ë¶„ì„ ëª¨ë“ˆì´ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
