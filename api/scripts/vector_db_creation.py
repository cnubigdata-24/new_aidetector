"""
### 1. ê°œì„ ëœ ë²¡í„°DB ìƒì„± ëª¨ë“ˆ - ì¥ì• ì‚¬ë¡€ ë°ì´í„°ë¥¼ ë²¡í„°DBë¡œ ë³€í™˜

ì´ ëª¨ë“ˆì€ JSON í˜•ì‹ì˜ ì¥ì• ì‚¬ë¡€ ë°ì´í„°ë¥¼ ì½ì–´ 
ChromaDB ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ë¡œ ìµœì í™”í•˜ì—¬ ì €ì¥í•©ë‹ˆë‹¤.
"""

import os
import json
import chromadb
from chromadb.utils import embedding_functions
import uuid
import shutil
import re
import logging
from tqdm import tqdm
import time
from concurrent.futures import ThreadPoolExecutor

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
)
logger = logging.getLogger(__name__)

# ìƒìˆ˜ ì •ì˜
VECTOR_DB_DIR = "./chroma_db"
VECTOR_DB_NEW_DIR = "./chroma_db_new"
RAG_DOCUMENT = r"D:\aidetector\static\rag_document\rag_data.json"
EMBEDDING_MODEL = "intfloat/multilingual-e5-base"  # ì„ë² ë”© ëª¨ë¸ ì„ íƒ
BATCH_SIZE = 20  # ë„ˆë¬´ í¬ë©´ ChromaDBì—ì„œ OOM ì—ëŸ¬ ë°œìƒ ê°€ëŠ¥

# ë¶„ì•¼ë³„ í‚¤ì›Œë“œ ë§µ - ì¶”ë¡  ê°œì„ ì„ ìœ„í•œ ì¶”ê°€ ë°ì´í„°
FIELD_KEYWORDS = {
    "IP": [
        "IP ë¶„ì•¼", "NMS", "Syslog", "SER", "OLT", "RN", "ONT", "Ntopia", 
        "ì—”í† í”¼ì•„", "MX960", "PE", "GS4K", "GS4000", "L2", "L3", 
        "Port Down", "í¬íŠ¸ ë‹¤ìš´", "SNMP", "OperStatus", "Ping ë¬´ì‘ë‹µ",
        "OSPF", "BGP", "PIM", "Neighbor", "CRC", "ì—ëŸ¬"
    ],
    "ì „ì†¡": [
        "ì „ì†¡ ë¶„ì•¼", "ROADM", "MSPP", "PTN", "POTN", "OTN", "AIS", "LOS", 
        "RDI", "CSF", "MUT_LOS", "OSC-LOS", "STM64_LOS", "AU-AIS", "GFP-FAIL",
        "MEP_LSP_LOC", "LINK-FAIL", "OPT-PWR-LOW", "MS-AIS", "TU-AIS"
    ],
    "êµí™˜": [
        "êµí™˜ ë¶„ì•¼", "AGW", "POTS", "SGW", "CGW", "A1930", "A6200", "A6000", 
        "A6010", "A6210", "A6220", "A6230", "A6231", "A6300", "UP0 LINK FAIL", 
        "UP LINK ALL FAIL", "LACP", "L3 DISCONNECTED", "TDXAGW DISCONNECTED",
        "T1 TIME OUT"
    ],
    "M/W": [
        "M/W", "MW", "ë§ˆì´í¬ë¡œì›¨ì´ë¸Œ", "ë§ˆì´í¬ë¡œ ì›¨ì´ë¸Œ", "MicroWave", "Micro Wave",
        "IP-20N", "í˜ì´ë”©", "Fading", "ì „íŒŒ", "CDM-SMR", "LOST CONTACT", "AIS-INSERT", 
        "DEMOD SYNC LOSS", "RF INPUT LOS", "TX LO", "RX LO", "Radio loss of frame",
        "Remote communication failure", "Loss of STM-1", "IDU", "ë„íŒŒê´€", "ê²°ë¹™",
        "ì•„ì´ì‹±", "icing"
    ],
    "ì„ ë¡œ": [
        "ì„ ë¡œ ë¶„ì•¼", "ì„ ë¡œ", "ê´‘ì¼€ì´ë¸”", "ì¼€ì´ë¸”", "í•œì „ ì¼€ì´ë¸”", "í•œì „ ê´‘ì¼€ì´ë¸”",
        "ì‚¬ì™¸ê³µì‚¬ì¥", "ì‚¬ì™¸ ê³µì‚¬ì¥", "ê´‘ì„ ë¡œ", "ê´‘ë ˆë²¨", "ê´‘ì í¼ ì½”ë“œ", "ì„ì°¨ê´‘",
        "ê´‘ì¼€ì´ë¸” í”¼í•´", "í•œì „ ì„ì°¨ê´‘", "ì ˆë‹¨", "ê´‘ì¼€ì´ë¸” ì ˆë‹¨", "ëŠê¹€"
    ],
    "ì „ì›": [
        "ì „ì› ë¶„ì•¼", "ë°œì „ê¸°", "UPS", "ë°°í„°ë¦¬", "ì¶•ì „ì§€", "MOF", "Fuse", "í“¨ì¦ˆ ì†Œì†",
        "í•œì „ ì •ì „", "CTTS", "CT", "PT", "ACB", "VCB", "ì •ì „", "ìƒì „"
    ],
}

def create_chroma_client():
    """ChromaDB í´ë¼ì´ì–¸íŠ¸ ìƒì„± ë° ì„¤ì •"""
    db_dir = VECTOR_DB_DIR
    try:
        if os.path.exists(db_dir):
            logger.info(f"ê¸°ì¡´ DB ë””ë ‰í† ë¦¬ ì œê±°: {db_dir}")
            shutil.rmtree(db_dir)
    except Exception as e:
        logger.error(f"ì£¼ DB ê²½ë¡œ ì˜¤ë¥˜: {e}, ëŒ€ì²´ ê²½ë¡œ ì‚¬ìš©")
        db_dir = VECTOR_DB_NEW_DIR

    os.makedirs(db_dir, exist_ok=True)
    logger.info(f"DB ë””ë ‰í† ë¦¬: {db_dir}")

    try:
        client = chromadb.PersistentClient(path=db_dir)
        try:
            client.delete_collection("nw_incidents")
        except:
            pass
        return client, db_dir
    except Exception as e:
        logger.error(f"Chroma í´ë¼ì´ì–¸íŠ¸ ì˜¤ë¥˜: {e}")
        return None, None


def extract_alert_codes(alert_text):
    """ê²½ë³´ ì½”ë“œ ì¶”ì¶œ - íŒ¨í„´ ë§¤ì¹­ ìµœì í™”"""
    if not alert_text:
        return []
        
    patterns = [
        r"[A-Z0-9]+_[A-Z0-9]+",  # ì–¸ë”ìŠ¤ì½”ì–´ í¬í•¨ ì½”ë“œ
        r"[A-Z0-9]+-[A-Z0-9-]+", # í•˜ì´í”ˆ í¬í•¨ ì½”ë“œ
        r"[A-Z][0-9]{4}",        # A ë‹¤ìŒ 4ìë¦¬ ìˆ«ì ì½”ë“œ
        r"(?:STM|OC)-\d+",       # STM/OC ê´€ë ¨ ì½”ë“œ
        r"UP\d+\s+LINK\s+(?:ALL\s+)?FAIL", # UP LINK FAIL íŒ¨í„´
        r"(?:Port|Link)\s+Down", # Port/Link Down íŒ¨í„´
    ]
    
    all_codes = set()
    for pattern in patterns:
        all_codes.update(re.findall(pattern, alert_text, re.IGNORECASE))
    
    return list(all_codes)[:20]  # ìµœëŒ€ 20ê°œ ì½”ë“œë§Œ ì‚¬ìš©


def identify_alert_fields(alert_text):
    """ê²½ë³´ í…ìŠ¤íŠ¸ì—ì„œ ê´€ë ¨ ë¶„ì•¼ ì‹ë³„"""
    if not alert_text:
        return ["ê¸°íƒ€"]
        
    field_scores = {}
    for field, keywords in FIELD_KEYWORDS.items():
        matches = sum(1 for kw in keywords if kw.lower() in alert_text.lower())
        if matches > 0:
            field_scores[field] = matches
    
    # ê°€ì¥ ë§ì´ ë§¤ì¹­ëœ ë¶„ì•¼ë“¤ ë°˜í™˜ (ìµœëŒ€ 3ê°œ)
    if field_scores:
        sorted_fields = sorted(field_scores.items(), key=lambda x: x[1], reverse=True)
        return [field for field, _ in sorted_fields[:3]]
    
    return ["ê¸°íƒ€"]


def extract_key_phrases(text, max_phrases=5):
    """ì¤‘ìš” êµ¬ë¬¸ ì¶”ì¶œ - ë¬¸ì„œ í‘œí˜„ë ¥ í–¥ìƒì„ ìœ„í•œ í‚¤ì›Œë“œ/êµ¬ë¬¸ ì¶”ì¶œ"""
    if not text:
        return []
        
    # ë¬¸ì¥ ë¶„ë¦¬
    sentences = re.split(r'[.!?]\s+', text)
    
    # ì¤‘ìš” êµ¬ë¬¸ í›„ë³´ë“¤
    patterns = [
        r'(?:ì¥ì• |ê³ ì¥|ë¶ˆëŸ‰|ì´ìƒ|ë¹„ì •ìƒ|ì ˆë‹¨|ë‹¨ì ˆ|í”¼í•´|ë¬¸ì œ|ì˜¤ë¥˜|error|fail|down|ë‹¤ìš´|ì‹¤íŒ¨|ì—ëŸ¬|ì¶©ëŒ|íŠ¸ëŸ¬ë¸”|Trouble|ë¬´ì‘ë‹µ|ê°ì†Œ|ì €í•˜|ì •ì „)(?:ê°€|ì´|ì€|ëŠ”)?\s+([ê°€-í£A-Za-z0-9_\-\s]+)',  # ì¥ì• /ë¶ˆëŸ‰ ê´€ë ¨ êµ¬ë¬¸
        r'([ê°€-í£A-Za-z0-9_\-\s]+)(?:ì—ì„œ|ì—)\s+(?:ì¥ì• |ê³ ì¥|ë¶ˆëŸ‰|ì´ìƒ|ë¹„ì •ìƒ|ì ˆë‹¨|ë‹¨ì ˆ|í”¼í•´|ë¬¸ì œ|ì˜¤ë¥˜|error|fail|down|ë‹¤ìš´|ì‹¤íŒ¨|ì—ëŸ¬|ì¶©ëŒ|íŠ¸ëŸ¬ë¸”|Trouble|ë¬´ì‘ë‹µ|ê°ì†Œ|ì €í•˜|ì •ì „)',        # ì¥ë¹„ ìœ„ì¹˜ ê´€ë ¨ êµ¬ë¬¸
        r'([A-Z0-9\-_]+)\s+(?:ì•ŒëŒ|ê²½ë³´|ì—ëŸ¬)(?:ê°€|ì´)?\s+ë°œìƒ',                     # íŠ¹ì • ì•ŒëŒ/ê²½ë³´ ê´€ë ¨ êµ¬ë¬¸
    ]
    
    phrases = set()
    for sentence in sentences:
        for pattern in patterns:
            matches = re.findall(pattern, sentence)
            for match in matches:
                if 3 <= len(match) <= 30:  # ì ì ˆí•œ ê¸¸ì´ì˜ êµ¬ë¬¸ë§Œ ì¶”ì¶œ
                    phrases.add(match.strip())
    
    # ê²°ê³¼ê°€ ì ìœ¼ë©´ í‚¤ì›Œë“œ ê¸°ë°˜ ì¶”ì¶œ
    if len(phrases) < 2:
        # ê¸°ê¸°ëª…, ê²½ë³´íƒ€ì… ë“± ì¤‘ìš” í‚¤ì›Œë“œ ê·¼ì²˜ ë¬¸êµ¬ ì¶”ì¶œ
        key_terms = [
            "OLT", "ROADM", "MSPP", "PTN", "M/W", "IDU", "ê´‘ì¼€ì´ë¸”", 
            "Port Down", "LOS", "AIS", "í˜ì´ë”©", "ê²°ë¹™", "ë°°í„°ë¦¬"
        ]
        for term in key_terms:
            if term in text:
                idx = text.find(term)
                start = max(0, idx - 15)
                end = min(len(text), idx + 25)
                phrases.add(text[start:end].strip())
    
    return list(phrases)[:max_phrases]


def create_metadata(fault_case):
    """ì¥ì•  ì‚¬ë¡€ì˜ ë©”íƒ€ë°ì´í„° ìƒì„± - ê²€ìƒ‰ê³¼ í•„í„°ë§ì„ ìœ„í•œ ë°ì´í„° ì¶”ê°€"""
    metadata = {
        "ì¥ì• ë²ˆí˜¸": fault_case.get("ì¥ì• ë²ˆí˜¸", ""),
        "ì¥ì• ëª…": fault_case.get("ì¥ì• ëª…", ""),
        "ì¥ì• ë¶„ì•¼": fault_case.get("ì¥ì• ë¶„ì•¼", ""),
        "ì¥ì• ì ": fault_case.get("ì¥ì• ì ", ""),
        "ë°œìƒì¼ì": fault_case.get("ë°œìƒì¼ì", ""),
        "êµ­ì‚¬": fault_case.get("êµ­ì‚¬", ""),
    }
    
    # ê²½ë³´ ê´€ë ¨ ë©”íƒ€ë°ì´í„° ì¶”ê°€
    alert_text = fault_case.get("ê²½ë³´í˜„í™©", "")
    if alert_text:
        metadata["ê²½ë³´ë¶„ì•¼"] = ", ".join(identify_alert_fields(alert_text))
        metadata["ê²½ë³´ì½”ë“œ"] = ", ".join(extract_alert_codes(alert_text))
        metadata["ê²½ë³´í˜„í™©"] = alert_text
    
    # ì¥ì•  ê´€ë ¨ ê¸°ë³¸ í•„ë“œ ì¶”ê°€
    for key in ["ì¥ì• ì ‘ìˆ˜ë‚´ì—­", "ì¥ì• ë¶„ì„", "ì¡°ì¹˜ë‚´ì—­"]:
        if fault_case.get(key):
            metadata[key] = fault_case[key]
            
            # í‚¤ì›Œë“œ ë°ì´í„° ì¶”ê°€ - ë‹¨ìˆœ í…ìŠ¤íŠ¸ ë§¤ì¹­ ê²€ìƒ‰ ê°•í™”ìš©
            if key == "ì¥ì• ì ‘ìˆ˜ë‚´ì—­":
                metadata["ì ‘ìˆ˜í‚¤ì›Œë“œ"] = ", ".join(extract_key_phrases(fault_case[key]))
            elif key == "ì¥ì• ë¶„ì„":
                metadata["ë¶„ì„í‚¤ì›Œë“œ"] = ", ".join(extract_key_phrases(fault_case[key]))
    
    return metadata


def create_documents(fault_case):
    """ê²€ìƒ‰ì„ ìœ„í•œ ë¬¸ì„œ ìƒì„± - ê° ì„¹ì…˜ë³„ ë¬¸ì„œì™€ í†µí•© ë¬¸ì„œ ìƒì„±"""
    def fmt_block(label, content):
        # ë¬¸ì„œ ì„¹ì…˜ í¬ë§·íŒ…
        return f"""{label}:
{content}

ì¥ì• ëª…: {fault_case['ì¥ì• ëª…']}
ì¥ì• ë¶„ì•¼: {fault_case['ì¥ì• ë¶„ì•¼']}
ì¥ì• ì : {fault_case['ì¥ì• ì ']}"""

    docs = []
    
    # í•µì‹¬ í•„ë“œë³„ ë¶„ë¦¬ ë¬¸ì„œ ìƒì„± (ì„¹ì…˜ë³„ ê²€ìƒ‰ ê°€ëŠ¥)
    for section in ["ì¥ì• ì ‘ìˆ˜ë‚´ì—­", "ê²½ë³´í˜„í™©", "ì¥ì• ë¶„ì„", "ì¡°ì¹˜ë‚´ì—­"]:
        if fault_case.get(section):
            docs.append(
                {"text": fmt_block(section, fault_case[section]), "type": section}
            )

    # ì „ì²´ ë¬¸ì„œë„ í•¨ê»˜ ìƒì„± (í†µí•© ê²€ìƒ‰ìš©)
    full_doc = f"""
ì¥ì• ë²ˆí˜¸: {fault_case.get('ì¥ì• ë²ˆí˜¸', '')}
ë°œìƒì¼ì: {fault_case.get('ë°œìƒì¼ì', '')}
ì¥ì• ëª…: {fault_case.get('ì¥ì• ëª…', '')}
êµ­ì‚¬: {fault_case.get('êµ­ì‚¬', '')}

ì¥ì• ì ‘ìˆ˜ë‚´ì—­:
{fault_case.get('ì¥ì• ì ‘ìˆ˜ë‚´ì—­', '')}

ê²½ë³´í˜„í™©:
{fault_case.get('ê²½ë³´í˜„í™©', '')}

ì¥ì• ë¶„ì•¼: {fault_case.get('ì¥ì• ë¶„ì•¼', '')}
ì¥ì• ì : {fault_case.get('ì¥ì• ì ', '')}

ì¥ì• ë¶„ì„:
{fault_case.get('ì¥ì• ë¶„ì„', '')}

ì¡°ì¹˜ë‚´ì—­:
{fault_case.get('ì¡°ì¹˜ë‚´ì—­', '')}
"""
    docs.append({"text": full_doc, "type": "full_document"})
    
    # ìš”ì•½ ë¬¸ì„œ ì¶”ê°€ (ë¹ ë¥¸ ê²€ìƒ‰ìš©)
    summary_doc = f"""
ì¥ì• ë²ˆí˜¸: {fault_case.get('ì¥ì• ë²ˆí˜¸', '')}
ì¥ì• ë¶„ì•¼: {fault_case.get('ì¥ì• ë¶„ì•¼', '')}
ì¥ì• ì : {fault_case.get('ì¥ì• ì ', '')}
êµ­ì‚¬: {fault_case.get('êµ­ì‚¬', '')}
ì¡°ì¹˜ìš”ì•½: {fault_case.get('ì¡°ì¹˜ë‚´ì—­', '')[:100]}
"""
    if fault_case.get('ê²½ë³´í˜„í™©'):
        codes = extract_alert_codes(fault_case['ê²½ë³´í˜„í™©'])
        if codes:
            summary_doc += f"\nì£¼ìš”ê²½ë³´ì½”ë“œ: {', '.join(codes[:5])}"
    
    docs.append({"text": summary_doc, "type": "summary"})
    
    return docs


def create_embedding_chunks(fault_cases):
    """íš¨ìœ¨ì ì¸ ì„ë² ë”©ì„ ìœ„í•œ ì²­í¬ ìƒì„±"""
    all_chunks = []
    
    for case in fault_cases:
        metadata = create_metadata(case)
        documents = create_documents(case)
        
        for doc in documents:
            doc_id = f"{case['ì¥ì• ë²ˆí˜¸']}_{doc['type']}_{uuid.uuid4()}"
            all_chunks.append({
                "id": doc_id,
                "text": doc["text"],
                "metadata": {**metadata, "doc_type": doc["type"]}
            })
    
    return all_chunks


def batch_save_to_db(collection, chunks, batch_size=BATCH_SIZE):
    """ë°°ì¹˜ ì²˜ë¦¬ë¥¼ í†µí•œ DB ì €ì¥ ìµœì í™”"""
    total_chunks = len(chunks)
    total_batches = (total_chunks + batch_size - 1) // batch_size
    
    for i in tqdm(range(0, total_chunks, batch_size), desc="ë²¡í„°DB ì €ì¥", total=total_batches):
        batch = chunks[i:i+batch_size]
        
        batch_ids = [item["id"] for item in batch]
        batch_docs = [item["text"] for item in batch]
        batch_metadatas = [item["metadata"] for item in batch]
        
        try:
            collection.add(
                ids=batch_ids,
                documents=batch_docs,
                metadatas=batch_metadatas,
            )
            logger.info(f"âœ… {len(batch_ids)}ê±´ ì €ì¥ ì™„ë£Œ")
        except Exception as e:
            logger.error(f"ğŸ”¥ ë°°ì¹˜ ì €ì¥ ì˜¤ë¥˜: {e}")
            time.sleep(1)  # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì ì‹œ ëŒ€ê¸°
            try:
                # ë‹¨ê±´ ì €ì¥ ì‹œë„
                for j in range(len(batch_ids)):
                    try:
                        collection.add(
                            ids=[batch_ids[j]],
                            documents=[batch_docs[j]],
                            metadatas=[batch_metadatas[j]],
                        )
                    except Exception as e2:
                        logger.error(f"ğŸ”¥ ë‹¨ê±´ ì €ì¥ ì˜¤ë¥˜: {e2}")
            except:
                logger.error("ë‹¨ê±´ ì €ì¥ë„ ì‹¤íŒ¨")
                pass


def preprocess_cases_parallel(fault_cases, max_workers=4):
    """ë³‘ë ¬ ì²˜ë¦¬ë¥¼ í†µí•œ ì‚¬ë¡€ ì „ì²˜ë¦¬ ìµœì í™”"""
    all_chunks = []
    
    def process_case(case):
        metadata = create_metadata(case)
        documents = create_documents(case)
        case_chunks = []
        
        for doc in documents:
            doc_id = f"{case['ì¥ì• ë²ˆí˜¸']}_{doc['type']}_{uuid.uuid4()}"
            case_chunks.append({
                "id": doc_id,
                "text": doc["text"],
                "metadata": {**metadata, "doc_type": doc["type"]}
            })
        
        return case_chunks
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        chunk_lists = list(executor.map(process_case, fault_cases))
    
    for chunk_list in chunk_lists:
        all_chunks.extend(chunk_list)
    
    return all_chunks


def optimize_collection(collection):
    """ë²¡í„° ì»¬ë ‰ì…˜ ìµœì í™” - ì¸ë±ìŠ¤ ì„¤ì •"""
    try:
        collection.create_index(
            index_type="hnsw",  # ëŒ€ìš©ëŸ‰ ë°ì´í„°ì— ì í•©í•œ ì¸ë±ìŠ¤
            params={"space_type": "cosine", "ef_construction": 200}
        )
        logger.info("âœ… ì»¬ë ‰ì…˜ ì¸ë±ìŠ¤ ìµœì í™” ì™„ë£Œ")
    except Exception as e:
        logger.warning(f"ì¸ë±ìŠ¤ ìµœì í™” ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œ ê°€ëŠ¥): {e}")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    start_time = time.time()
    
    # 1. JSON íŒŒì¼ ë¡œë“œ
    json_path = RAG_DOCUMENT
    if not os.path.exists(json_path):
        logger.error(f"JSON íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {json_path}")
        return

    # 2. ChromaDB í´ë¼ì´ì–¸íŠ¸ ìƒì„±
    client, db_dir = create_chroma_client()
    if not client:
        return

    # 3. ì„ë² ë”© í•¨ìˆ˜ ì„¤ì •
    ef = embedding_functions.SentenceTransformerEmbeddingFunction(
        model_name=EMBEDDING_MODEL
    )

    # 4. ì»¬ë ‰ì…˜ ìƒì„±
    try:
        collection = client.create_collection(
            name="nw_incidents",
            embedding_function=ef,
            metadata={"description": "í†µì‹ ì¥ë¹„ ì¥ì• ì‚¬ë¡€ ë°ì´í„°"},
        )
    except:
        collection = client.get_collection(name="nw_incidents", embedding_function=ef)

    # 5. ë°ì´í„° ë¡œë“œ
    try:
        with open(json_path, "r", encoding="utf-8-sig") as f:
            fault_cases = json.load(f)
            logger.info(f"ì¥ì• ì‚¬ë¡€ {len(fault_cases)}ê±´ ë¡œë“œ ì™„ë£Œ")
    except Exception as e:
        logger.error(f"JSON ë¡œë“œ ì˜¤ë¥˜: {e}")
        return

    # 6. ë³‘ë ¬ ì²˜ë¦¬ë¡œ ì²­í¬ ìƒì„±
    all_chunks = preprocess_cases_parallel(fault_cases)
    logger.info(f"ì´ {len(all_chunks)}ê°œ ì²­í¬ ìƒì„± ì™„ë£Œ")

    # 7. ë°°ì¹˜ ì²˜ë¦¬ë¡œ DB ì €ì¥
    batch_save_to_db(collection, all_chunks, BATCH_SIZE)

    # 8. ì»¬ë ‰ì…˜ ìµœì í™”
    optimize_collection(collection)

    end_time = time.time()
    total_time = end_time - start_time
    logger.info(f"ì´ {len(all_chunks)}ê±´ ë¬¸ì„œê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. DB ìœ„ì¹˜: {db_dir}")
    logger.info(f"ì²˜ë¦¬ ì‹œê°„: {total_time:.2f}ì´ˆ")


if __name__ == "__main__":
    main()